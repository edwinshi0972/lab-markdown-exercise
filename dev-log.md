Edwin Shi 
7/2/25

## What I Learned Today
- ### LLMs
	- #### Tokenization
		- Turn text into tokens 
		- Byte-pair encoding
	- #### Inference
		- Trained model is fed a prompt and has to infer based on the given tokens what the *next* token is
	- #### Context Window
		- How much of the conversation the LLM can use as information to inform it's next inference
	- #### Emergent Properties
		- properties that the LLM exhibits *post-training* that the creators have **no explanation for**

## Lab Reflection
> I think something I found challenging was figuring out how to get my obsidian notes onto github. ChatGPT really helped me figure out that I needed to create a repo within my obsidian vault and add, commit, and push to github from there.

## Code Snippet
``` python 
num1 = input("Please enter the first number") 
num2 = input("Please enter the second number") 
``` 

## Helpful Links
1. [TA Instructions](https://gist.github.com/iitgrad/0f01ca89bee00f8ced3ace0ff68702dc)
2. [ChatGPT Setup](https://gist.github.com/iitgrad/50e31acfa5824de3b54e4b6856a50bb5)






